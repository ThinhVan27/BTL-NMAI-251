{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chess\n",
                "import numpy as np\n",
                "from chess_env import ChessEnv\n",
                "from rl_agent import RLAgent\n",
                "from random_agent import RandomAgent\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_agent(agent, opponent, num_games=20):\n",
                "    wins = 0\n",
                "    draws = 0\n",
                "    losses = 0\n",
                "    \n",
                "    # Force exploitation during evaluation\n",
                "    original_epsilon = agent.epsilon\n",
                "    agent.epsilon = 0.0\n",
                "    \n",
                "    for _ in range(num_games):\n",
                "        env = ChessEnv()\n",
                "        state = env.reset()\n",
                "        done = False\n",
                "        board = env.board\n",
                "        \n",
                "        while not done:\n",
                "            if board.turn == chess.WHITE:\n",
                "                legal_moves = env.get_legal_actions()\n",
                "                action_idx = agent.get_action(state, legal_moves)\n",
                "                move = env.decode_action(action_idx)\n",
                "                \n",
                "                if move not in board.legal_moves:\n",
                "                    losses += 1; done = True; break\n",
                "                board.push(move)\n",
                "            else:\n",
                "                if board.is_game_over(): break\n",
                "                move = opponent.get_action(board)\n",
                "                board.push(move)\n",
                "            \n",
                "            state = env.get_state() # Update state for next step\n",
                "            \n",
                "            if board.is_game_over():\n",
                "                outcome = board.outcome()\n",
                "                if outcome.winner == chess.WHITE: wins += 1\n",
                "                elif outcome.winner == chess.BLACK: losses += 1\n",
                "                else: draws += 1\n",
                "                done = True\n",
                "\n",
                "    agent.epsilon = original_epsilon\n",
                "    return wins / num_games"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "load_model_path = None # Example: \"models/rl_model_1000.pth\"\n",
                "start_episode = 0\n",
                "\n",
                "if load_model_path:\n",
                "    start_episode = int(load_model_path.split(\"_\")[-1].split(\".\")[0])\n",
                "\n",
                "episodes = 5000\n",
                "target_update_freq = 20\n",
                "\n",
                "# Initialize Environment and Agents\n",
                "env = ChessEnv()\n",
                "agent = RLAgent()\n",
                "\n",
                "if load_model_path:\n",
                "    agent.load(load_model_path, True)\n",
                "    print(f\"Loaded model from {load_model_path}, starting at episode {start_episode}\")\n",
                "\n",
                "# CRITICAL CHANGE: Train against RandomAgent first!\n",
                "train_opponent = RandomAgent()\n",
                "eval_opponent = RandomAgent()\n",
                "\n",
                "if not os.path.exists(\"models\"): os.makedirs(\"models\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for episode in range(start_episode, episodes):\n",
                "    try:\n",
                "        pass # Placeholder for keyboard interrupt check structure if needed, but in notebook we can just stop cell\n",
                "    except KeyboardInterrupt:\n",
                "        print(f\"\\nTraining interrupted. Saving model at episode {episode}...\")\n",
                "        agent.save(f\"models/rl_model_{episode}.pth\")\n",
                "        break\n",
                "\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    max_steps = 200 # Games vs Random shouldn't take forever\n",
                "    step_count = 0\n",
                "    total_reward = 0\n",
                "    \n",
                "    while not done and step_count < max_steps:\n",
                "        step_count += 1\n",
                "        \n",
                "        # --- Agent Turn (White) ---\n",
                "        legal_moves = env.get_legal_actions()\n",
                "        action_idx = agent.get_action(state, legal_moves)\n",
                "        \n",
                "        next_state, reward, done, info = env.step(action_idx)\n",
                "        # total_reward += reward\n",
                "        \n",
                "        # --- Opponent Turn (Black) ---\n",
                "        if not done:\n",
                "            opp_move = train_opponent.get_action(env.board)\n",
                "            opp_action_idx = env.encode_action(opp_move)\n",
                "            \n",
                "            # We care about the state AFTER opponent moves\n",
                "            next_state_final, opp_reward, done, info = env.step(opp_action_idx)\n",
                "            \n",
                "            # Reward Logic:\n",
                "            # My Reward - Opponent Gain. \n",
                "            # If Opponent blunders (negative opp_reward), I get a bonus.\n",
                "            opp_pure_reward = opp_reward - env.weights['step_penalty']\n",
                "            if opp_pure_reward > 0:\n",
                "                combined_reward = reward - opp_pure_reward\n",
                "            else:\n",
                "                combined_reward = reward\n",
                "            \n",
                "            agent.remember(state, action_idx, combined_reward, next_state_final, done)\n",
                "            state = next_state_final\n",
                "            total_reward += combined_reward\n",
                "        else:\n",
                "            agent.remember(state, action_idx, reward, next_state, done)\n",
                "            state = next_state # Technically terminal\n",
                "            total_reward += reward\n",
                "        \n",
                "        agent.update()\n",
                "\n",
                "        \n",
                "    if episode % target_update_freq == 0:\n",
                "        agent.update_target_network()\n",
                "    agent.decay_epsilon()\n",
                "        \n",
                "    print(f\"Episode: {episode}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
                "    \n",
                "    # Evaluation every 100 episodes\n",
                "    if episode > 0 and episode % 100 == 0:\n",
                "        win_rate = evaluate_agent(agent, eval_opponent, num_games=50)\n",
                "        print(f\"--- Eval Episode {episode}: Win Rate {win_rate*100:.1f}% ---\")\n",
                "        \n",
                "        if win_rate >= 0.90:\n",
                "            print(\"GOAL REACHED! Saving model.\")\n",
                "            agent.save(f\"models/chess_90_percent.pth\")\n",
                "            # Optional: break or switch to harder opponent here\n",
                "\n",
                "    if episode % 250 == 0:\n",
                "        agent.save(f\"models/chess_{episode}.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}