{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import chess\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import random\n",
                "from collections import deque\n",
                "import os\n",
                "from chess import Move, Board"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base Agent Class\n",
                "class Agent:\n",
                "    def __init__(self):\n",
                "        pass\n",
                "\n",
                "    def get_action(self, game_state: Board):\n",
                "        pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chess Environment\n",
                "class ChessEnv:\n",
                "    def __init__(self):\n",
                "        self.board = chess.Board()\n",
                "        self.action_space_size = 4096\n",
                "        \n",
                "        self.weights = {\n",
                "            'pawn': 1.0,\n",
                "            'knight': 3.0,\n",
                "            'bishop': 3.2,\n",
                "            'rook': 5.0,\n",
                "            'queen': 9.0,\n",
                "            'king_safety': 0.0,      # Disable for now (noise)\n",
                "            'mobility': 0.0,         # Disable for now (noise)\n",
                "            'center': 0.0,           # Disable for now (noise)\n",
                "            'pst_scale': 0.0,        # Disable PST initially to focus on pure material capture\n",
                "            'step_penalty': -0.05,   # Small pressure to finish games\n",
                "            'check': 1.0,            # Helpful tactile feedback\n",
                "            'castling': 1.0,         # Good safe habit\n",
                "            'repetition_penalty': -2.0 \n",
                "        }\n",
                "        \n",
                "        # Simple Piece-Square Tables (Pawn & Knight)\n",
                "        # Scaled 0-100, centered on mid-game principles\n",
                "        self.pst_pawn = [\n",
                "             0,  0,  0,  0,  0,  0,  0,  0,\n",
                "            50, 50, 50, 50, 50, 50, 50, 50,\n",
                "            10, 10, 20, 30, 30, 20, 10, 10,\n",
                "             5,  5, 10, 25, 25, 10,  5,  5,\n",
                "             0,  0,  0, 20, 20,  0,  0,  0,\n",
                "             5, -5,-10,  0,  0,-10, -5,  5,\n",
                "             5, 10, 10,-20,-20, 10, 10,  5,\n",
                "             0,  0,  0,  0,  0,  0,  0,  0\n",
                "        ]\n",
                "        self.pst_knight = [\n",
                "            -50,-40,-30,-30,-30,-30,-40,-50,\n",
                "            -40,-20,  0,  0,  0,  0,-20,-40,\n",
                "            -30,  0, 10, 15, 15, 10,  0,-30,\n",
                "            -30,  5, 15, 20, 20, 15,  5,-30,\n",
                "            -30,  0, 15, 20, 20, 15,  0,-30,\n",
                "            -30,  5, 10, 15, 15, 10,  5,-30,\n",
                "            -40,-20,  0,  5,  5,  0,-20,-40,\n",
                "            -50,-40,-30,-30,-30,-30,-40,-50\n",
                "        ]\n",
                "\n",
                "    def reset(self):\n",
                "        self.board.reset()\n",
                "        return self.get_state()\n",
                "\n",
                "    def step(self, action_idx):\n",
                "        move = self.decode_action(action_idx)\n",
                "        if move not in self.board.legal_moves:\n",
                "            return self.get_state(), -10.0, True, {\"legal\": False} # Penalty for illegal\n",
                "\n",
                "        # 1. Calculate Potential BEFORE move\n",
                "        prev_potential = self._get_potential(self.board)\n",
                "        \n",
                "        # Check for castling bonus (before move)\n",
                "        castling_bonus = 0.0\n",
                "        if self.board.is_castling(move):\n",
                "            castling_bonus = self.weights['castling']\n",
                "\n",
                "        # 2. Execute Move\n",
                "        self.board.push(move)\n",
                "        done = self.board.is_game_over()\n",
                "\n",
                "        # 3. Calculate Potential AFTER move\n",
                "        curr_potential = self._get_potential(self.board)\n",
                "        \n",
                "        # Check bonus (after move, is opponent in check?)\n",
                "        check_bonus = 0.0\n",
                "        if self.board.is_check():\n",
                "            check_bonus = self.weights['check']\n",
                "            \n",
                "        # Repetition Penalty\n",
                "        repetition_penalty = 0.0\n",
                "        if self.board.is_repetition(2): # 2-fold repetition\n",
                "            repetition_penalty = self.weights['repetition_penalty']\n",
                "\n",
                "        # 4. Reward Shaping (Difference in Potential)\n",
                "        # If I am white, I want potential to increase.\n",
                "        # If I am black, I want potential to decrease (since eval is usually White-centric)\n",
                "        # Note: self.board.turn is now the OPPONENT's turn after push.\n",
                "        # So if we just moved White, board.turn is Black.\n",
                "        \n",
                "        # We need the color of the agent who JUST moved.\n",
                "        agent_color = not self.board.turn \n",
                "        \n",
                "        # Standard RL perspective: Reward is for the Agent.\n",
                "        diff = curr_potential - prev_potential\n",
                "        reward = diff if agent_color == chess.WHITE else -diff\n",
                "        \n",
                "        # Add Bonuses and Penalties\n",
                "        reward += castling_bonus\n",
                "        reward += check_bonus\n",
                "        reward += repetition_penalty\n",
                "        \n",
                "        # Add Step Penalty (Time pressure)\n",
                "        reward += self.weights['step_penalty']\n",
                "\n",
                "        # Terminal Rewards (Override shaping for clear outcomes)\n",
                "        if done:\n",
                "            if self.board.is_checkmate():\n",
                "                # Massive reward for winning.\n",
                "                reward += 100.0 \n",
                "                # reward += 20.0\n",
                "            elif self.board.is_stalemate() or self.board.is_insufficient_material():\n",
                "                # Draw is better than losing, but worse than winning.\n",
                "                reward += 0.0\n",
                "\n",
                "        return self.get_state(), reward, done, {\"legal\": True}\n",
                "\n",
                "    def _get_potential(self, board):\n",
                "        \"\"\"\n",
                "        Calculates a dense evaluation of the board from White's perspective.\n",
                "        \"\"\"\n",
                "        score = 0\n",
                "        \n",
                "        # 1. Material\n",
                "        pm = board.piece_map()\n",
                "        for sq, piece in pm.items():\n",
                "            val = 0\n",
                "            if piece.piece_type == chess.PAWN: val = self.weights['pawn']\n",
                "            elif piece.piece_type == chess.KNIGHT: val = self.weights['knight']\n",
                "            elif piece.piece_type == chess.BISHOP: val = self.weights['bishop']\n",
                "            elif piece.piece_type == chess.ROOK: val = self.weights['rook']\n",
                "            elif piece.piece_type == chess.QUEEN: val = self.weights['queen']\n",
                "            \n",
                "            # PST\n",
                "            pst_val = 0\n",
                "            if self.weights['pst_scale'] > 0:\n",
                "                # Calculate rank/file (0-7)\n",
                "                rank = chess.square_rank(sq)\n",
                "                file = chess.square_file(sq)\n",
                "                idx = (7 - rank) * 8 + file # Map to 0=a8, 63=h1 table layout\n",
                "                \n",
                "                # Mirror for black\n",
                "                if piece.color == chess.BLACK:\n",
                "                    rank = 7 - rank # Mirror rank\n",
                "                    idx = (7 - rank) * 8 + file\n",
                "\n",
                "                if piece.piece_type == chess.PAWN: pst_val = self.pst_pawn[idx]\n",
                "                elif piece.piece_type == chess.KNIGHT: pst_val = self.pst_knight[idx]\n",
                "                \n",
                "            total_piece_val = val + (pst_val * 0.01 * self.weights['pst_scale'])\n",
                "            \n",
                "            if piece.color == chess.WHITE:\n",
                "                score += total_piece_val\n",
                "            else:\n",
                "                score -= total_piece_val\n",
                "\n",
                "        # 2. Center Control (Bonus for occupying center)\n",
                "        # Simple check for pieces on e4, d4, e5, d5\n",
                "        for sq in [chess.E4, chess.D4, chess.E5, chess.D5]:\n",
                "            p = board.piece_at(sq)\n",
                "            if p:\n",
                "                if p.color == chess.WHITE: score += self.weights['center']\n",
                "                else: score -= self.weights['center']\n",
                "\n",
                "        # 3. Mobility (Number of legal moves)\n",
                "        # Note: python-chess legal_moves is for the current turn side only.\n",
                "        # Approximating mobility is expensive if we flip turns. \n",
                "        # For efficiency, we might skip or just use current side.\n",
                "        # Here is a simple implementation adding current side mobility:\n",
                "        legal_count = board.legal_moves.count()\n",
                "        mobility_score = legal_count * self.weights['mobility']\n",
                "        \n",
                "        if board.turn == chess.WHITE:\n",
                "            score += mobility_score\n",
                "        else:\n",
                "            score -= mobility_score\n",
                "            \n",
                "        return score\n",
                "\n",
                "    def get_state(self):\n",
                "        \"\"\"\n",
                "        Converts board to 12x8x8 numpy array.\n",
                "        Channels:\n",
                "        0-5: White P, N, B, R, Q, K\n",
                "        6-11: Black P, N, B, R, Q, K\n",
                "        \"\"\"\n",
                "        state = np.zeros((12, 8, 8), dtype=np.float32)\n",
                "        \n",
                "        piece_map = {\n",
                "            chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n",
                "            chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5\n",
                "        }\n",
                "        \n",
                "        for square in chess.SQUARES:\n",
                "            piece = self.board.piece_at(square)\n",
                "            if piece:\n",
                "                rank = chess.square_rank(square)\n",
                "                file = chess.square_file(square)\n",
                "                \n",
                "                channel = piece_map[piece.piece_type]\n",
                "                if piece.color == chess.BLACK:\n",
                "                    channel += 6\n",
                "                \n",
                "                state[channel, rank, file] = 1\n",
                "                \n",
                "        return state\n",
                "\n",
                "    def encode_action(self, move: chess.Move) -> int:\n",
                "        \"\"\"Encodes a chess move into an integer 0-4095.\"\"\"\n",
                "        from_square = move.from_square\n",
                "        to_square = move.to_square\n",
                "        return from_square * 64 + to_square\n",
                "\n",
                "    def decode_action(self, action_idx: int) -> chess.Move:\n",
                "        from_square = action_idx // 64\n",
                "        to_square = action_idx % 64\n",
                "        move = chess.Move(from_square, to_square)\n",
                "        \n",
                "        # --- FIX: Auto-promote to Queen ---\n",
                "        # Check if this is a pawn move to the last rank\n",
                "        piece = self.board.piece_at(from_square)\n",
                "        if piece and piece.piece_type == chess.PAWN:\n",
                "            rank = chess.square_rank(to_square)\n",
                "            if (piece.color == chess.WHITE and rank == 7) or \\\n",
                "            (piece.color == chess.BLACK and rank == 0):\n",
                "                move.promotion = chess.QUEEN # Auto-promote\n",
                "                \n",
                "        return move\n",
                "\n",
                "    def get_legal_actions(self):\n",
                "        \"\"\"Returns list of legal action indices.\"\"\"\n",
                "        legal_moves = []\n",
                "        for move in self.board.legal_moves:\n",
                "            legal_moves.append(self.encode_action(move))\n",
                "        return legal_moves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Agent\n",
                "class RandomAgent(Agent):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        \n",
                "    def get_action(self, game_state: Board) -> Move:\n",
                "        valid_moves = game_state.legal_moves\n",
                "        return random.choice(list(valid_moves))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RL Agent (Dueling DQN)\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, channels):\n",
                "        super(ResidualBlock, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(channels)\n",
                "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
                "        self.bn2 = nn.BatchNorm2d(channels)\n",
                "\n",
                "    def forward(self, x):\n",
                "        residual = x\n",
                "        x = torch.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.bn2(self.conv2(x))\n",
                "        x += residual\n",
                "        return torch.relu(x)\n",
                "\n",
                "class DuelingDQN(nn.Module):\n",
                "    def __init__(self, input_shape, num_actions):\n",
                "        super(DuelingDQN, self).__init__()\n",
                "        self.input_shape = input_shape\n",
                "        self.num_actions = num_actions\n",
                "        \n",
                "        # Initial Conv Layer\n",
                "        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(64)\n",
                "        \n",
                "        # Residual Towers (AlphaZero style, but smaller)\n",
                "        self.res1 = ResidualBlock(64)\n",
                "        self.res2 = ResidualBlock(64)\n",
                "        self.res3 = ResidualBlock(64)\n",
                "        self.res4 = ResidualBlock(64)\n",
                "        \n",
                "        flat_size = 64 * 8 * 8\n",
                "        \n",
                "        # Value Stream (Evaluates the board state)\n",
                "        self.value_fc = nn.Sequential(\n",
                "            nn.Linear(flat_size, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(512, 1)\n",
                "        )\n",
                "        \n",
                "        # Advantage Stream (Evaluates each specific action)\n",
                "        self.advantage_fc = nn.Sequential(\n",
                "            nn.Linear(flat_size, 1024),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(1024, num_actions)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = torch.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.res1(x)\n",
                "        x = self.res2(x)\n",
                "        x = self.res3(x)\n",
                "        x = self.res4(x)\n",
                "        \n",
                "        x = x.view(x.size(0), -1)\n",
                "        \n",
                "        value = self.value_fc(x)\n",
                "        advantage = self.advantage_fc(x)\n",
                "        \n",
                "        # Dueling Network Aggregation\n",
                "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
                "        return q_values\n",
                "\n",
                "class RLAgent(Agent):\n",
                "    def __init__(self, state_shape=(12, 8, 8), action_size=4096):\n",
                "        super().__init__()\n",
                "        self.state_shape = state_shape\n",
                "        self.action_size = action_size\n",
                "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        \n",
                "        self.policy_net = DuelingDQN(state_shape, action_size).to(self.device)\n",
                "        self.target_net = DuelingDQN(state_shape, action_size).to(self.device)\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        self.target_net.eval()\n",
                "        \n",
                "        # Lower LR for stability with ResNet\n",
                "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-4)\n",
                "        self.memory = deque(maxlen=20000) # Increased memory\n",
                "        \n",
                "        self.batch_size = 64 # Increased batch size\n",
                "        self.gamma = 0.99\n",
                "        self.epsilon = 1.0\n",
                "        self.epsilon_min = 0.05\n",
                "        # Faster decay initially to exploit learned behavior sooner\n",
                "        self.epsilon_decay = 0.9995 \n",
                "        \n",
                "    def get_action(self, game_state, legal_moves_indices=None):\n",
                "        is_inference = False\n",
                "        board_for_inference = None\n",
                "\n",
                "        if isinstance(game_state, chess.Board):\n",
                "            is_inference = True\n",
                "            board_for_inference = game_state.copy()\n",
                "            state_tensor = self._board_to_tensor(game_state)\n",
                "            legal_moves_indices = self._get_legal_actions(game_state)\n",
                "        else:\n",
                "            state_tensor = game_state\n",
                "        \n",
                "        if not is_inference and random.random() < self.epsilon:\n",
                "            if legal_moves_indices:\n",
                "                return random.choice(legal_moves_indices)\n",
                "            return random.randint(0, self.action_size - 1)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            state_tensor = torch.FloatTensor(state_tensor).unsqueeze(0).to(self.device)\n",
                "            q_values = self.policy_net(state_tensor)\n",
                "            \n",
                "            if legal_moves_indices:\n",
                "                # Mask illegal moves with negative infinity\n",
                "                mask = torch.full((1, self.action_size), -float('inf')).to(self.device)\n",
                "                mask[0, legal_moves_indices] = 0\n",
                "                q_values += mask\n",
                "                \n",
                "            action_idx = q_values.argmax().item()\n",
                "        \n",
                "        if is_inference:\n",
                "            return self._decode_action(action_idx, board_for_inference)\n",
                "            \n",
                "        return action_idx\n",
                "\n",
                "    def update(self):\n",
                "        if len(self.memory) < self.batch_size:\n",
                "            return\n",
                "        \n",
                "        batch = random.sample(self.memory, self.batch_size)\n",
                "        state, action, reward, next_state, done = zip(*batch)\n",
                "        \n",
                "        state = torch.FloatTensor(np.array(state)).to(self.device)\n",
                "        action = torch.LongTensor(action).unsqueeze(1).to(self.device)\n",
                "        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
                "        next_state = torch.FloatTensor(np.array(next_state)).to(self.device)\n",
                "        done = torch.FloatTensor(done).unsqueeze(1).to(self.device)\n",
                "        \n",
                "        # Double DQN Logic\n",
                "        # 1. Select best action using Policy Net\n",
                "        next_actions = self.policy_net(next_state).argmax(1, keepdim=True)\n",
                "        # 2. Evaluate that action using Target Net\n",
                "        next_q_values = self.target_net(next_state).gather(1, next_actions)\n",
                "        \n",
                "        expected_q_values = reward + (1 - done) * self.gamma * next_q_values\n",
                "        q_values = self.policy_net(state).gather(1, action)\n",
                "        \n",
                "        loss = nn.MSELoss()(q_values, expected_q_values)\n",
                "        \n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        # Gradient clipping to prevent explosion\n",
                "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
                "        self.optimizer.step()\n",
                "        \n",
                "\n",
                "    def decay_epsilon(self):\n",
                "        if self.epsilon > self.epsilon_min:\n",
                "            self.epsilon *= self.epsilon_decay\n",
                "\n",
                "    def update_target_network(self):\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "\n",
                "    def remember(self, state, action, reward, next_state, done):\n",
                "        self.memory.append((state, action, reward, next_state, done))\n",
                "\n",
                "    def save(self, path):\n",
                "        torch.save(self.policy_net.state_dict(), path)\n",
                "\n",
                "    def load(self, path, training=False):\n",
                "        self.policy_net.load_state_dict(torch.load(path, map_location=self.device))\n",
                "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
                "        self.epsilon = 0.5 if training else 0.0\n",
                "        if training: self.policy_net.train()\n",
                "        else: self.policy_net.eval()\n",
                "\n",
                "    def _board_to_tensor(self, board):\n",
                "        state = np.zeros((12, 8, 8), dtype=np.float32)\n",
                "        piece_map = {\n",
                "            chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n",
                "            chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5\n",
                "        }\n",
                "        for square in chess.SQUARES:\n",
                "            piece = board.piece_at(square)\n",
                "            if piece:\n",
                "                rank = chess.square_rank(square)\n",
                "                file = chess.square_file(square)\n",
                "                channel = piece_map[piece.piece_type]\n",
                "                if piece.color == chess.BLACK:\n",
                "                    channel += 6\n",
                "                state[channel, rank, file] = 1\n",
                "        return state\n",
                "\n",
                "    def _get_legal_actions(self, board):\n",
                "        legal_moves = []\n",
                "        for move in board.legal_moves:\n",
                "            legal_moves.append(move.from_square * 64 + move.to_square)\n",
                "        return legal_moves\n",
                "\n",
                "    def _decode_action(self, action_idx: int, board: chess.Board = None) -> chess.Move:\n",
                "        from_square = action_idx // 64\n",
                "        to_square = action_idx % 64\n",
                "        move = chess.Move(from_square, to_square)\n",
                "        \n",
                "        # Logic to auto-promote during inference/testing\n",
                "        if board:\n",
                "            piece = board.piece_at(from_square)\n",
                "            if piece and piece.piece_type == chess.PAWN:\n",
                "                rank = chess.square_rank(to_square)\n",
                "                if (piece.color == chess.WHITE and rank == 7) or \\\n",
                "                   (piece.color == chess.BLACK and rank == 0):\n",
                "                    move.promotion = chess.QUEEN\n",
                "        \n",
                "        return move"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation Function\n",
                "def evaluate_agent(agent, opponent, num_games=20):\n",
                "    wins = 0\n",
                "    draws = 0\n",
                "    losses = 0\n",
                "    \n",
                "    # Force exploitation during evaluation\n",
                "    original_epsilon = agent.epsilon\n",
                "    agent.epsilon = 0.0\n",
                "    \n",
                "    for _ in range(num_games):\n",
                "        env = ChessEnv()\n",
                "        state = env.reset()\n",
                "        done = False\n",
                "        board = env.board\n",
                "        \n",
                "        while not done:\n",
                "            if board.turn == chess.WHITE:\n",
                "                legal_moves = env.get_legal_actions()\n",
                "                action_idx = agent.get_action(state, legal_moves)\n",
                "                move = env.decode_action(action_idx)\n",
                "                \n",
                "                if move not in board.legal_moves:\n",
                "                    losses += 1; done = True; break\n",
                "                board.push(move)\n",
                "            else:\n",
                "                if board.is_game_over(): break\n",
                "                move = opponent.get_action(board)\n",
                "                board.push(move)\n",
                "            \n",
                "            state = env.get_state() # Update state for next step\n",
                "            \n",
                "            if board.is_game_over():\n",
                "                outcome = board.outcome()\n",
                "                if outcome.winner == chess.WHITE: wins += 1\n",
                "                elif outcome.winner == chess.BLACK: losses += 1\n",
                "                else: draws += 1\n",
                "                done = True\n",
                "\n",
                "    agent.epsilon = original_epsilon\n",
                "    return wins / num_games"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration & Initialization\n",
                "load_model_path = None # Example: \"models/rl_model_1000.pth\"\n",
                "start_episode = 0\n",
                "\n",
                "if load_model_path:\n",
                "    start_episode = int(load_model_path.split(\"_\")[-1].split(\".\")[0])\n",
                "\n",
                "episodes = 5000\n",
                "target_update_freq = 20\n",
                "\n",
                "# Initialize Environment and Agents\n",
                "env = ChessEnv()\n",
                "agent = RLAgent()\n",
                "\n",
                "if load_model_path:\n",
                "    agent.load(load_model_path, True)\n",
                "    print(f\"Loaded model from {load_model_path}, starting at episode {start_episode}\")\n",
                "\n",
                "# CRITICAL CHANGE: Train against RandomAgent first!\n",
                "train_opponent = RandomAgent()\n",
                "eval_opponent = RandomAgent()\n",
                "\n",
                "if not os.path.exists(\"models\"): os.makedirs(\"models\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training Loop\n",
                "for episode in range(start_episode, episodes):\n",
                "    try:\n",
                "        pass # Placeholder for keyboard interrupt check structure if needed, but in notebook we can just stop cell\n",
                "    except KeyboardInterrupt:\n",
                "        print(f\"\\nTraining interrupted. Saving model at episode {episode}...\")\n",
                "        agent.save(f\"models/rl_model_{episode}.pth\")\n",
                "        break\n",
                "\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    max_steps = 200 # Games vs Random shouldn't take forever\n",
                "    step_count = 0\n",
                "    total_reward = 0\n",
                "    \n",
                "    while not done and step_count < max_steps:\n",
                "        step_count += 1\n",
                "        \n",
                "        # --- Agent Turn (White) ---\n",
                "        legal_moves = env.get_legal_actions()\n",
                "        action_idx = agent.get_action(state, legal_moves)\n",
                "        \n",
                "        next_state, reward, done, info = env.step(action_idx)\n",
                "        # total_reward += reward\n",
                "        \n",
                "        # --- Opponent Turn (Black) ---\n",
                "        if not done:\n",
                "            opp_move = train_opponent.get_action(env.board)\n",
                "            opp_action_idx = env.encode_action(opp_move)\n",
                "            \n",
                "            # We care about the state AFTER opponent moves\n",
                "            next_state_final, opp_reward, done, info = env.step(opp_action_idx)\n",
                "            \n",
                "            # Reward Logic:\n",
                "            # My Reward - Opponent Gain. \n",
                "            # If Opponent blunders (negative opp_reward), I get a bonus.\n",
                "            opp_pure_reward = opp_reward - env.weights['step_penalty']\n",
                "            if opp_pure_reward > 0:\n",
                "                combined_reward = reward - opp_pure_reward\n",
                "            else:\n",
                "                combined_reward = reward\n",
                "            \n",
                "            agent.remember(state, action_idx, combined_reward, next_state_final, done)\n",
                "            state = next_state_final\n",
                "            total_reward += combined_reward\n",
                "        else:\n",
                "            agent.remember(state, action_idx, reward, next_state, done)\n",
                "            state = next_state # Technically terminal\n",
                "            total_reward += reward\n",
                "        \n",
                "        agent.update()\n",
                "\n",
                "        \n",
                "    if episode % target_update_freq == 0:\n",
                "        agent.update_target_network()\n",
                "    agent.decay_epsilon()\n",
                "        \n",
                "    print(f\"Episode: {episode}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
                "    \n",
                "    # Evaluation every 100 episodes\n",
                "    if episode > 0 and episode % 100 == 0:\n",
                "        win_rate = evaluate_agent(agent, eval_opponent, num_games=50)\n",
                "        print(f\"--- Eval Episode {episode}: Win Rate {win_rate*100:.1f}% ---\")\n",
                "        \n",
                "        if win_rate >= 0.90:\n",
                "            print(\"GOAL REACHED! Saving model.\")\n",
                "            agent.save(f\"models/chess_90_percent.pth\")\n",
                "            # Optional: break or switch to harder opponent here\n",
                "\n",
                "    if episode % 250 == 0:\n",
                "        agent.save(f\"models/chess_{episode}.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
